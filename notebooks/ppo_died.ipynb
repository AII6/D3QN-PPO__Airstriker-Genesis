{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo_one.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "id": "Yhik4sjct8J-",
        "outputId": "38e05b0f-958a-4151-df08-962fbbf215ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-retro\n",
            "  Downloading gym_retro-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (162.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 162.0 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from gym-retro) (1.5.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-retro) (0.17.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.3.0)\n",
            "Installing collected packages: gym-retro\n",
            "Successfully installed gym-retro-0.8.0\n"
          ]
        }
      ],
      "source": [
        "#@title 默认标题文本\n",
        "!pip install gym-retro\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import retro\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = retro.make(game='Airstriker-Genesis')"
      ],
      "metadata": {
        "id": "5UCpF4F-wI7-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Actor_NN, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(8, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(16, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        )\n",
        "\n",
        "        # self.cnn = models.vgg19(pretrained=True).features.to(DEVICE).eval().requires_grad_(requires_grad=False)\n",
        "        self.pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.BatchNorm1d(128, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.BatchNorm1d(32, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 6),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x2 = x.permute(0, 3, 2, 1)\n",
        "        y = self.cnn(x2)\n",
        "        y1 = self.pool(y)\n",
        "        y1 = torch.flatten(y1, start_dim=1, end_dim=-1)\n",
        "        z = self.fc(y1)\n",
        "        return z\n",
        "\n",
        "\n",
        "class Critic_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Critic_NN, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(8, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "            nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(16, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        )\n",
        "\n",
        "        # self.cnn = models.vgg19(pretrained=True).features.to(DEVICE).eval().requires_grad_(requires_grad=False)\n",
        "        self.pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(784, 64),\n",
        "            nn.BatchNorm1d(64, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 16),\n",
        "            nn.BatchNorm1d(16, affine=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x2 = x.permute(0, 3, 2, 1)\n",
        "        y = self.cnn(x2)\n",
        "        y1 = self.pool(y)\n",
        "        y1 = torch.flatten(y1, start_dim=1, end_dim=-1)\n",
        "        z = self.fc(y1)\n",
        "        return z\n"
      ],
      "metadata": {
        "id": "sloWiVTfwMQ1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO(nn.Module):\n",
        "    def __init__(self, timestep, discount, batch, device, lr, lamb, actor_update, critic_update):\n",
        "        super(PPO, self).__init__()\n",
        "        self.actor_update = actor_update\n",
        "        self.critic_update = critic_update\n",
        "        self.lamb = lamb  \n",
        "        self.lr = lr\n",
        "        self.device = device\n",
        "        self.batch_size = batch\n",
        "        self.timestep = timestep\n",
        "        self.gamma = discount\n",
        "        self.ji = self.lamb * self.gamma\n",
        "        self.coeff = torch.tensor([self.ji**i for i in range(timestep)]).to(self.device)  # 算advantage\n",
        "        self.target_coeff = torch.tensor([self.gamma**i for i in range(timestep)]).to(device) # 算target，折扣因子\n",
        "        self.epsilon = 0.2\n",
        "        self.actor = Actor_NN().to(self.device)\n",
        "        self.old_actor = Actor_NN().to(self.device)\n",
        "        self.critic = Critic_NN().to(self.device)\n",
        "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=self.lr)\n",
        "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=self.lr)\n",
        "        self.discrete_action = []\n",
        "        self.decode_action()\n",
        "\n",
        "    def decode_action(self):\n",
        "        buttons = env.unwrapped.buttons\n",
        "        combos = [[],['B'],['LEFT'],['RIGHT'],['B','LEFT'],['B','RIGHT']]\n",
        "        for combo in combos:\n",
        "            # arr = np.array([0] * env.action_space.n)\n",
        "            arr = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "            for button in combo:\n",
        "                arr[buttons.index(button)] = 1\n",
        "            self.discrete_action.append(arr)\n",
        "        # self.discrete_action = np.array(self.discrete_action)\n",
        "\n",
        "    def action_select(self, s):\n",
        "        s = (torch.FloatTensor(s)).to(self.device)\n",
        "        prob = self.actor(s)\n",
        "        dist = torch.distributions.Categorical(prob.squeeze(0))\n",
        "        action = self.discrete_action[dist.sample()]\n",
        "        return action\n",
        "\n",
        "\n",
        "    def actor_learn(self, s, a, advantage):\n",
        "\n",
        "        prob = self.actor(s[:self.batch_size])\n",
        "        pi = torch.distributions.Categorical(prob)\n",
        "\n",
        "        old_prob = self.old_actor(s[:self.batch_size])\n",
        "        old_pi = torch.distributions.Categorical(old_prob)\n",
        "        # print(self.discrete_action)\n",
        "        # print(a)\n",
        "        a = torch.tensor([self.discrete_action.index(list(i.cpu())) for i in a]).to(self.device)\n",
        "        # print(a)\n",
        "        # print(pi.log_prob(a))\n",
        "        # print(old_pi.log_prob(a))\n",
        "        ratio = torch.exp(pi.log_prob(a) - old_pi.log_prob(a))\n",
        "\n",
        "        advantage = torch.tensor(advantage).to(self.device)\n",
        "        surr = (ratio * advantage).reshape(-1,1)  # torch.Size([batch, 1])\n",
        "\n",
        "        # mean取均值，就是期望\n",
        "        # 这个loss就是画面价值，越高越好\n",
        "        loss = -torch.mean(\n",
        "            torch.min(surr, torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage.reshape(-1, 1)))\n",
        "\n",
        "        self.actor_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.actor_optim.step()\n",
        "\n",
        "    def critic_learn(self, targets, s): # 传入s重新计算V而不用self.V一个是因为actor反向传播后会清空梯度，\n",
        "                      # 在这里就无法反向传播了，再一个就是多次反向传播每次都会清空梯度\n",
        "        \n",
        "        self.V = self.critic(s).reshape(self.batch_size+self.timestep)\n",
        "        targets_ = torch.FloatTensor(targets).to(self.device)\n",
        "\n",
        "        loss_func = nn.MSELoss()\n",
        "        # print(targets_.shape)\n",
        "        # print(self.V[:self.batch_size].shape)\n",
        "        loss = loss_func(self.V[:self.batch_size], targets_)\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "    def target_cal(self, r):  # 用于更新critic\n",
        "        targets = []\n",
        "        for i in range(self.batch_size):\n",
        "          # print(r[i].device)\n",
        "          # print(self.V.device)\n",
        "          # print(self.target_coeff.device)\n",
        "          target = r[i] + torch.sum(self.V[i+1:i+self.timestep+1,0]*self.target_coeff)\n",
        "          targets.append(int(target))\n",
        "        \n",
        "        return targets\n",
        "\n",
        "    def delta_cal(self, r, s):  \n",
        "\n",
        "        self.V = self.critic(s)\n",
        "        delta = []\n",
        "        for i in range(len(self.V)-1):\n",
        "            delta0 = r[i] + self.gamma * self.V[i+1,0] - self.V[i,0]\n",
        "            delta.append(int(delta0))\n",
        "\n",
        "        return delta  # 用来计算advantage\n",
        "\n",
        "    def adv_cal(self, delta):\n",
        "\n",
        "        advantage = []\n",
        "        delta_ = torch.tensor(delta).to(self.device)\n",
        "        for i in range(self.batch_size):\n",
        "          adv = torch.sum(self.coeff * delta_[i:i+self.timestep])\n",
        "          advantage.append(int(adv))\n",
        "\n",
        "        return advantage\n",
        "\n",
        "    def update(self, s, a, delta, targets):\n",
        "        self.old_actor.load_state_dict(self.actor.state_dict())\n",
        "        advantage = self.adv_cal(delta)\n",
        "\n",
        "        for i in range(self.actor_update):\n",
        "            self.actor_learn(s, a, advantage)\n",
        "\n",
        "        for i in range(self.critic_update):\n",
        "            self.critic_learn(targets, s)"
      ],
      "metadata": {
        "id": "ZPj5C7l4wPBV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda')\n",
        "BATCH = 64\n",
        "EPSILON = 0.98\n",
        "DISCOUNT = 0.99\n",
        "TIME_STEP = 256\n",
        "GAE_PARA = 0.95\n",
        "CLIPING = 0.1\n",
        "N_EPISODE = 721\n",
        "LR = 2.5*1e-4\n",
        "Actor_Update = 3\n",
        "Critic_Update = 3"
      ],
      "metadata": {
        "id": "JmAQT1SwrTlY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.seed(23)\n",
        "torch.manual_seed(23)\n",
        "\n",
        "agent = PPO(TIME_STEP, DISCOUNT, BATCH, DEVICE, LR, \n",
        "            GAE_PARA, Actor_Update, Critic_Update).to(DEVICE)"
      ],
      "metadata": {
        "id": "20RUNE4xao_U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.load_state_dict(torch.load(\"/content/ppo_480.pth\"))"
      ],
      "metadata": {
        "id": "FHP_NT2iatKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a480fdba-d577-4114-ead8-a8d97d190af8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "kmeans_died = joblib.load(\"/content/k_means_died\")\n",
        "# kmeans_left = joblib.load(\"/content/k_means_left\")\n",
        "# kmeans_right = joblib.load(\"/content/k_means_right\")\n",
        "\n",
        "all_ep_r = []\n",
        "for i in range(N_EPISODE):\n",
        "    ep_r = 0\n",
        "    s = env.reset()[24:, :, :]\n",
        "    # env.render()\n",
        "    states, actions, rewards = [], [], []\n",
        "    done = False\n",
        "    t = 0\n",
        "    life_index = 0\n",
        "    life = np.empty(2)\n",
        "    while done is False:\n",
        "        a = agent.eval().action_select(torch.tensor(s, dtype=torch.float).unsqueeze(0))\n",
        "        s_, r, done, _ = env.step(a)\n",
        "        # 存储前后两帧的生死信息\n",
        "        if_died = kmeans_died.predict([s_[128:135, 120:207, :].flatten()])\n",
        "        # print(if_died)\n",
        "        # 0是活着，1是死了\n",
        "        index = life_index % 2\n",
        "        life[index] = if_died\n",
        "        life_index += 1\n",
        "\n",
        "        # # 检测是否在左右两个角落，修改奖励来避免一直躲在角落\n",
        "        # right = kmeans_right.predict([s_[180:220, 260:310, :].flatten()])\n",
        "        # left = kmeans_left.predict([s_[180:220, 5:55, :].flatten()])\n",
        "        # # 0是最边上的角落里，1是不在那个小角落\n",
        "\n",
        "        s_ = s_[24:, :, :]\n",
        "\n",
        "        # 修改奖励(每帧存活就有奖励+2，死亡一条命-500，躲在角落里每帧-1)\n",
        "        if r > 0:\n",
        "            r = 80  # 打掉飞机奖励改为80\n",
        "        elif life[0] == 0 and life[1] == 0:\n",
        "            r = 1\n",
        "        elif life[(index + 1) % 2] == 0 and life[index] == 1:  # 检测两帧的，防止死亡画面过长一直减分数\n",
        "            r = -500\n",
        "        # if not (right and left):\n",
        "        #     r -= 2\n",
        "\n",
        "        ep_r += r\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = s_\n",
        "\n",
        "        t += 1  # 加完之后t为存储的个数\n",
        "\n",
        "        if t % (BATCH + TIME_STEP) == 0:  # 括号里不减一是因为算delta还要算下一时刻的V\n",
        "            \n",
        "            states = torch.FloatTensor(np.array(states)).to(DEVICE)   # list里是ndarray的s，先全换成ndarray，再换成tensor，快很多很多\n",
        "            actions = torch.tensor(actions[:agent.batch_size]).to(DEVICE)\n",
        "            rewards = torch.tensor(rewards).to(DEVICE)\n",
        "            \n",
        "            delta = agent.delta_cal(rewards, states)\n",
        "            targets = agent.target_cal(rewards)\n",
        "            \n",
        "            # 计算delta，截断版本的advantage\n",
        "            agent.train().update(states, actions, delta, targets)  # 进行actor和critic网络的更新\n",
        "            states, actions, rewards = [], [], []\n",
        "\n",
        "    print('Episode {:03d} | Reward:{:.03f}'.format(i, ep_r))\n",
        "    if i % 40 == 0:\n",
        "        torch.save(agent.state_dict(),\"/content/ppo_\"+str(i)+\".pth\")\n",
        "    if i == 481:\n",
        "        all_ep_r.append(ep_r)\n",
        "    else:\n",
        "        all_ep_r.append(all_ep_r[-1] * 0.9 + ep_r * 0.1)  # 平滑\n",
        "\n",
        "plt.plot(np.arange(len(all_ep_r)), all_ep_r)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZuKWqhMQwQdh",
        "outputId": "a133b3b6-6ccb-4303-adb8-b15c01d05409"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 481 | Reward:275.000\n",
            "Episode 482 | Reward:206.000\n",
            "Episode 483 | Reward:462.000\n",
            "Episode 484 | Reward:1345.000\n",
            "Episode 485 | Reward:30.000\n",
            "Episode 486 | Reward:-395.000\n",
            "Episode 487 | Reward:-460.000\n",
            "Episode 488 | Reward:-332.000\n",
            "Episode 489 | Reward:-219.000\n",
            "Episode 490 | Reward:15.000\n",
            "Episode 491 | Reward:-29.000\n",
            "Episode 492 | Reward:-105.000\n",
            "Episode 493 | Reward:-336.000\n",
            "Episode 494 | Reward:414.000\n",
            "Episode 495 | Reward:619.000\n",
            "Episode 496 | Reward:-518.000\n",
            "Episode 497 | Reward:20.000\n",
            "Episode 498 | Reward:-76.000\n",
            "Episode 499 | Reward:686.000\n",
            "Episode 500 | Reward:-634.000\n",
            "Episode 501 | Reward:-126.000\n",
            "Episode 502 | Reward:113.000\n",
            "Episode 503 | Reward:-258.000\n",
            "Episode 504 | Reward:-24.000\n",
            "Episode 505 | Reward:-149.000\n",
            "Episode 506 | Reward:924.000\n",
            "Episode 507 | Reward:550.000\n",
            "Episode 508 | Reward:676.000\n",
            "Episode 509 | Reward:10.000\n",
            "Episode 510 | Reward:20.000\n",
            "Episode 511 | Reward:214.000\n",
            "Episode 512 | Reward:553.000\n",
            "Episode 513 | Reward:-110.000\n",
            "Episode 514 | Reward:-324.000\n",
            "Episode 515 | Reward:-15.000\n",
            "Episode 516 | Reward:285.000\n",
            "Episode 517 | Reward:664.000\n",
            "Episode 518 | Reward:48.000\n",
            "Episode 519 | Reward:-448.000\n",
            "Episode 520 | Reward:-107.000\n",
            "Episode 521 | Reward:124.000\n",
            "Episode 522 | Reward:278.000\n",
            "Episode 523 | Reward:116.000\n",
            "Episode 524 | Reward:196.000\n",
            "Episode 525 | Reward:555.000\n",
            "Episode 526 | Reward:717.000\n",
            "Episode 527 | Reward:1306.000\n",
            "Episode 528 | Reward:572.000\n",
            "Episode 529 | Reward:664.000\n",
            "Episode 530 | Reward:98.000\n",
            "Episode 531 | Reward:274.000\n",
            "Episode 532 | Reward:407.000\n",
            "Episode 533 | Reward:389.000\n",
            "Episode 534 | Reward:-368.000\n",
            "Episode 535 | Reward:-574.000\n",
            "Episode 536 | Reward:114.000\n",
            "Episode 537 | Reward:360.000\n",
            "Episode 538 | Reward:247.000\n",
            "Episode 539 | Reward:465.000\n",
            "Episode 540 | Reward:1721.000\n",
            "Episode 541 | Reward:6365.000\n",
            "Episode 542 | Reward:537.000\n",
            "Episode 543 | Reward:87.000\n",
            "Episode 544 | Reward:1073.000\n",
            "Episode 545 | Reward:117.000\n",
            "Episode 546 | Reward:-314.000\n",
            "Episode 547 | Reward:817.000\n",
            "Episode 548 | Reward:98.000\n",
            "Episode 549 | Reward:1739.000\n",
            "Episode 550 | Reward:1169.000\n",
            "Episode 551 | Reward:-175.000\n",
            "Episode 552 | Reward:447.000\n",
            "Episode 553 | Reward:735.000\n",
            "Episode 554 | Reward:796.000\n",
            "Episode 555 | Reward:265.000\n",
            "Episode 556 | Reward:-290.000\n",
            "Episode 557 | Reward:1455.000\n",
            "Episode 558 | Reward:-274.000\n",
            "Episode 559 | Reward:2181.000\n",
            "Episode 560 | Reward:1525.000\n",
            "Episode 561 | Reward:707.000\n",
            "Episode 562 | Reward:1891.000\n",
            "Episode 563 | Reward:1200.000\n",
            "Episode 564 | Reward:645.000\n",
            "Episode 565 | Reward:1047.000\n",
            "Episode 566 | Reward:5736.000\n",
            "Episode 567 | Reward:1136.000\n",
            "Episode 568 | Reward:-387.000\n",
            "Episode 569 | Reward:3801.000\n",
            "Episode 570 | Reward:4087.000\n",
            "Episode 571 | Reward:768.000\n",
            "Episode 572 | Reward:4201.000\n",
            "Episode 573 | Reward:5074.000\n",
            "Episode 574 | Reward:5619.000\n",
            "Episode 575 | Reward:234.000\n",
            "Episode 576 | Reward:4544.000\n",
            "Episode 577 | Reward:775.000\n",
            "Episode 578 | Reward:6968.000\n",
            "Episode 579 | Reward:3402.000\n",
            "Episode 580 | Reward:4589.000\n",
            "Episode 581 | Reward:6698.000\n",
            "Episode 582 | Reward:6492.000\n",
            "Episode 583 | Reward:-370.000\n",
            "Episode 584 | Reward:4153.000\n",
            "Episode 585 | Reward:5345.000\n",
            "Episode 586 | Reward:562.000\n",
            "Episode 587 | Reward:564.000\n",
            "Episode 588 | Reward:554.000\n",
            "Episode 589 | Reward:781.000\n",
            "Episode 590 | Reward:557.000\n",
            "Episode 591 | Reward:-349.000\n",
            "Episode 592 | Reward:1303.000\n",
            "Episode 593 | Reward:3773.000\n",
            "Episode 594 | Reward:495.000\n",
            "Episode 595 | Reward:1051.000\n",
            "Episode 596 | Reward:851.000\n",
            "Episode 597 | Reward:93.000\n",
            "Episode 598 | Reward:3898.000\n",
            "Episode 599 | Reward:1381.000\n",
            "Episode 600 | Reward:5665.000\n",
            "Episode 601 | Reward:5292.000\n",
            "Episode 602 | Reward:5003.000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3da49fa7fa80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlife\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0ms_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# 存储前后两帧的生死信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f768d478f1fa>\u001b[0m in \u001b[0;36maction_select\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(len(all_ep_r)), all_ep_r)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "YXc819W7KU9o",
        "outputId": "9a10bc0b-c93d-4d54-dbcc-c17004306a3e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SwcZF-JXKaAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}